{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0_03SoxXRBn"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfmOaIWZ4bYQ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zF_fnCZ7w4TI"
      },
      "outputs": [],
      "source": [
        "!pip install SPARQLWrapper\n",
        "!pip install faiss-cpu\n",
        "!pip install rdflib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXMO4lGAMjnu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import re\n",
        "import json\n",
        "import requests\n",
        "from typing import Dict, Optional\n",
        "import faiss\n",
        "from faiss import IndexFlatIP\n",
        "from rdflib import Graph, URIRef, BNode\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "import requests\n",
        "from urllib.parse import quote\n",
        "import warnings\n",
        "from urllib3.exceptions import InsecureRequestWarning\n",
        "from collections import defaultdict\n",
        "import t2sparql_dbpedia_prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2t0f3rdLUAw"
      },
      "source": [
        "# Тестирование Spotlight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJJtnL04y3eg"
      },
      "outputs": [],
      "source": [
        "# Конфигурация\n",
        "api_key = \"\"\n",
        "API_BASE_URL = \"https://api.dbpedia-spotlight.org\"\n",
        "DATASET_URL = \"https://raw.githubusercontent.com/Perevalov/QALD_9_plus/refs/heads/main/data/qald_9_plus_train_dbpedia.json\"\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# Загрузка датасета QALD-9\n",
        "def load_dataset():\n",
        "    response = requests.get(DATASET_URL)\n",
        "    response.raise_for_status()\n",
        "    return json.loads(response.text)\n",
        "\n",
        "# Сравнение результатов с датасетом\n",
        "def compare_with_dataset(api_uris, dataset_uris):\n",
        "    api_uris = set(api_uris)\n",
        "    dataset_uris = set(dataset_uris)\n",
        "\n",
        "    correct = api_uris & dataset_uris\n",
        "    incorrect = api_uris - dataset_uris\n",
        "    missing = dataset_uris - api_uris\n",
        "\n",
        "    precision = len(correct) / len(api_uris) if api_uris else 0\n",
        "    recall = len(correct) / len(dataset_uris) if dataset_uris else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"correct\": len(correct),\n",
        "        \"incorrect\": len(incorrect),\n",
        "        \"missing\": len(missing),\n",
        "        \"api_uris\": api_uris,\n",
        "        \"dataset_uris\": dataset_uris\n",
        "    }\n",
        "\n",
        "\n",
        "def translate_to_english(text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a precise translation engine. Translate the input to English exactly, preserving: \"\n",
        "                            \"1. All named entities (names, places, titles) \"\n",
        "                            \"2. Technical terms \"\n",
        "                            \"3. Question structure \"\n",
        "                            \"Output ONLY the translation without commentary.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Translate this to English exactly:\\n\\n{text}\"\n",
        "            }\n",
        "        ],\n",
        "        temperature=0.1,\n",
        "        max_tokens=500\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def get_dbpedia(text: str, language: str = \"en\") -> Optional[Dict]:\n",
        "    \"\"\"Get entities from DBpedia Spotlight API\"\"\"\n",
        "\n",
        "    # Отключаем предупреждения о небезопасных запросах\n",
        "    requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
        "\n",
        "    endpoint = f\"https://api.dbpedia-spotlight.org/{language}/annotate\"\n",
        "    headers = {\"Accept\": \"application/json\"}\n",
        "    params = {\"text\": text, \"confidence\": 0.5}\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            endpoint,\n",
        "            headers=headers,\n",
        "            data=params,\n",
        "            verify=False  # Disable SSL verification\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"DBpedia Spotlight API error: {e}\")\n",
        "        return None\n",
        "\n",
        "def uris(rare_question: str) -> Tuple[Optional[str], Dict[str, str]]:\n",
        "    \"\"\"New entity extraction using DBpedia Spotlight\"\"\"\n",
        "\n",
        "    #print('Question: ', rare_question)\n",
        "    en_question = translate_to_english(rare_question)\n",
        "    #print('Translated question:', en_question)\n",
        "\n",
        "    spotlight_result = get_dbpedia(en_question)\n",
        "\n",
        "    if spotlight_result and \"Resources\" in spotlight_result:\n",
        "        entities = {}\n",
        "        tagged_parts = []\n",
        "        remaining_text = en_question\n",
        "\n",
        "        # Process each found entity\n",
        "        for resource in sorted(spotlight_result[\"Resources\"], key=lambda x: -int(x[\"@offset\"])):\n",
        "            entity_text = resource[\"@surfaceForm\"]\n",
        "            entity_type = resource[\"@types\"].split(\",\")[0].split(\":\")[-1] if resource[\"@types\"] else \"thing\"\n",
        "            entity_uri = resource['@URI']\n",
        "\n",
        "            # Replace in remaining text\n",
        "            if entity_text in remaining_text:\n",
        "                entities[entity_text] = entity_uri\n",
        "                tagged_parts.append((remaining_text.index(entity_text),\n",
        "                                    f\"<{entity_text}>\"))\n",
        "                remaining_text = remaining_text.replace(entity_text, \"\", 1)\n",
        "\n",
        "        # Reconstruct tagged question\n",
        "        if tagged_parts:\n",
        "            # Sort by original position\n",
        "            tagged_parts.sort()\n",
        "            tagged_question = \"\"\n",
        "            last_pos = 0\n",
        "\n",
        "            for pos, tag in tagged_parts:\n",
        "                tagged_question += remaining_text[last_pos:pos] + tag\n",
        "                last_pos = pos\n",
        "\n",
        "            tagged_question += remaining_text[last_pos:]\n",
        "\n",
        "            return tagged_question, entities\n",
        "    else:\n",
        "      print('ERROR')\n",
        "      # original extracting + generating uris\n",
        "      return '', {}\n",
        "\n",
        "\n",
        "\n",
        "# Обработка одного вопроса\n",
        "def process_question(question):\n",
        "    results = {}\n",
        "\n",
        "    # Получаем ожидаемые URI из датасета\n",
        "    dataset_uris = [binding[\"uri\"][\"value\"] for binding in question[\"answers\"][0][\"results\"][\"bindings\"]]\n",
        "\n",
        "    tagged_question, entitiy_uris = uris(question['question'][1]['string'])\n",
        "    print('Tagged question: ', tagged_question)\n",
        "    print('dataset uris: ', dataset_uris)\n",
        "    print('Entity uris: ', entitiy_uris)\n",
        "\n",
        "    # 4. Сравниваем с датасетом\n",
        "    comparison = compare_with_dataset(entitiy_uris.keys(), dataset_uris)\n",
        "    results[\"comparison\"] = comparison\n",
        "\n",
        "    return {\n",
        "        \"id\": question[\"id\"],\n",
        "        \"question\": tagged_question,\n",
        "        \"results\": results\n",
        "    }\n",
        "\n",
        "# Основная функция тестирования\n",
        "def test_spotlight_api(max_questions=None):\n",
        "    # Загружаем датасет\n",
        "    dataset = load_dataset()\n",
        "    questions = dataset[\"questions\"]\n",
        "\n",
        "    if max_questions:\n",
        "        questions = questions[:max_questions]\n",
        "\n",
        "    total_stats = defaultdict(float)\n",
        "    question_count = 0\n",
        "\n",
        "    for question in questions:\n",
        "        question_result = process_question(question)\n",
        "        if not question_result:\n",
        "            continue\n",
        "\n",
        "        question_count += 1\n",
        "        qid = question_result[\"id\"]\n",
        "        comparison = question_result[\"results\"][\"comparison\"]\n",
        "\n",
        "        # Выводим результаты для каждого вопроса\n",
        "        print(f\"\\nQuestion ID: {qid}\")\n",
        "        print(f\"Precision: {comparison['precision']:.2f}, Recall: {comparison['recall']:.2f}, F1: {comparison['f1']:.2f}\")\n",
        "        print(f\"Correct: {comparison['correct']}, Incorrect: {comparison['incorrect']}, Missing: {comparison['missing']}\")\n",
        "\n",
        "        # Суммируем статистику\n",
        "        for metric in [\"precision\", \"recall\", \"f1\", \"correct\", \"incorrect\", \"missing\"]:\n",
        "            total_stats[metric] += comparison[metric]\n",
        "\n",
        "        print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
        "\n",
        "    # Выводим итоговую статистику\n",
        "    if question_count > 0:\n",
        "        print(\"\\n=== FINAL STATISTICS ===\")\n",
        "        print(f\"Total questions processed: {question_count}\")\n",
        "        for metric in [\"precision\", \"recall\", \"f1\"]:\n",
        "            avg = total_stats[metric] / question_count\n",
        "            print(f\"Average {metric}: {avg:.2f}\")\n",
        "\n",
        "        print(f\"\\nTotal correct entities: {int(total_stats['correct'])}\")\n",
        "        print(f\"Total incorrect entities: {int(total_stats['incorrect'])}\")\n",
        "        print(f\"Total missing entities: {int(total_stats['missing'])}\")\n",
        "\n",
        "# Запуск тестирования\n",
        "if __name__ == \"__main__\":\n",
        "    test_spotlight_api(max_questions=10)  # Тестируем первые 10 вопросов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-u9S4g_7jDw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n12g4GWTiH3b"
      },
      "outputs": [],
      "source": [
        "get_dbpedia('What types of animals are on the verge of extinction?')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0AC-ND8al8h"
      },
      "source": [
        "# DIN SPARQL [DBPEDIA]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8-yEQbCW1lA"
      },
      "outputs": [],
      "source": [
        "class RAGSystem:\n",
        "    def __init__(self, dataset_paths: List[str]):\n",
        "        self.datasets = self._load_datasets(dataset_paths)\n",
        "        self.all_data = self._preprocess_data()\n",
        "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self._build_index()\n",
        "\n",
        "    def _load_datasets(self, paths: List[str]) -> List[Dict]:\n",
        "        \"\"\"Загрузка всех датасетов из JSON файлов\"\"\"\n",
        "        datasets = []\n",
        "        for path in paths:\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                # Обработка разных форматов датасетов\n",
        "                if isinstance(data, dict) and 'questions' in data:\n",
        "                    datasets.append(data)  # QALD-формат\n",
        "                elif isinstance(data, list):\n",
        "                    datasets.extend(data)  # LC-QuAD формат (список вопросов)\n",
        "        return datasets\n",
        "\n",
        "    def _preprocess_data(self) -> List[Dict]:\n",
        "        \"\"\"Объединение и предобработка данных из разных датасетов\"\"\"\n",
        "        processed_data = []\n",
        "\n",
        "        for dataset in self.datasets:\n",
        "\n",
        "            if isinstance(dataset, dict) and 'questions' in dataset:\n",
        "                # Обработка QALD-формата\n",
        "                for question in dataset['questions']:\n",
        "                    # Берем первый английский вопрос или первый вопрос любого языка\n",
        "                    en_questions = [q['string'] for q in question['question'] if q['language'] == 'en']\n",
        "                    question_text = en_questions[0] if en_questions else question['question'][0]['string']\n",
        "                    processed_item = {\n",
        "                        'question': question_text,\n",
        "                        'query': question.get('query', {}).get('sparql', ''),\n",
        "                        'dataset': 'qald',\n",
        "                        'id': question.get('id', ''),\n",
        "                        'languages': [q['language'] for q in question['question']]\n",
        "                    }\n",
        "                    processed_data.append(processed_item)\n",
        "\n",
        "            elif isinstance(dataset, dict) and '_id' in dataset:\n",
        "                # Обработка LC-QuAD формата (отдельные вопросы)\n",
        "\n",
        "                processed_item = {\n",
        "                    'question': dataset.get('corrected_question', ''),\n",
        "                    'query': dataset.get('sparql_query', ''),\n",
        "                    'dataset': 'lc_quad',\n",
        "                    'id': dataset.get('_id', '')\n",
        "                }\n",
        "                processed_data.append(processed_item)\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def _build_index(self):\n",
        "        \"\"\"Построение FAISS индекса для векторного поиска\"\"\"\n",
        "        questions = [item['question'] for item in self.all_data]\n",
        "        self.question_embeddings = self.model.encode(questions)\n",
        "\n",
        "        dimension = self.question_embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatL2(dimension)\n",
        "        self.index.add(self.question_embeddings)\n",
        "\n",
        "    def query(self, question: str, top_k: int = 3, threshold: Optional[float] = None,\n",
        "              dataset_filter: Optional[str] = None, language: str = 'en') -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Поиск наиболее релевантных вопросов\n",
        "        \"\"\"\n",
        "        query_embedding = self.model.encode([question])\n",
        "        distances, indices = self.index.search(query_embedding, top_k)\n",
        "\n",
        "        results = []\n",
        "        for idx, dist in zip(indices[0], distances[0]):\n",
        "            similarity = 1 - dist\n",
        "            if threshold is None or similarity >= threshold:\n",
        "                result = self.all_data[idx].copy()\n",
        "                result['similarity_score'] = float(similarity)\n",
        "                if dataset_filter is None or result['dataset'].lower() == dataset_filter.lower():\n",
        "                    if language != 'en' and 'languages' in result:\n",
        "                        if language not in result['languages']:\n",
        "                            continue\n",
        "                    results.append(result)\n",
        "\n",
        "        return sorted(results, key=lambda x: x['similarity_score'], reverse=True)\n",
        "\n",
        "    def get_context(self, question: str, top_k: int = 3, threshold: Optional[float] = None,\n",
        "                    dataset_filter: Optional[str] = None, language: str = 'en') -> str:\n",
        "        \"\"\"\n",
        "        Получение контекста в виде текста для заданного вопроса\n",
        "        \"\"\"\n",
        "        similar_items = self.query(\n",
        "            question,\n",
        "            top_k=top_k,\n",
        "            threshold=threshold,\n",
        "            dataset_filter=dataset_filter,\n",
        "            language=language\n",
        "        )\n",
        "\n",
        "        context = \"\"\n",
        "        for item in similar_items:\n",
        "            context += f\"Question: {item['question']}\\n\"\n",
        "            if item.get('query'):\n",
        "                context += f\"SPARQL: {item['query']}\\n\"\n",
        "            context += \"\\n\"\n",
        "        return context.strip()\n",
        "\n",
        "    def get_datasets_info(self) -> Dict:\n",
        "        \"\"\"Получение информации о загруженных датасетах\"\"\"\n",
        "        qald_count = sum(1 for item in self.all_data if item['dataset'] == 'qald')\n",
        "        lc_quad_count = sum(1 for item in self.all_data if item['dataset'] == 'lc_quad')\n",
        "\n",
        "        return {\n",
        "            'qald': {\n",
        "                'size': qald_count,\n",
        "                'description': 'QALD dataset with multilingual questions and SPARQL queries'\n",
        "            },\n",
        "            'lc_quad': {\n",
        "                'size': lc_quad_count,\n",
        "                'description': 'LC-QuAD dataset with English questions and SPARQL templates'\n",
        "            }\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baXVA7672mvE"
      },
      "outputs": [],
      "source": [
        "class DBpediaPipeline:\n",
        "    def __init__(self, api_key: str, dbpedia_endpoint: str = \"http://dbpedia.org/sparql\",\n",
        "                 NER_PROMPT: str = t2sparql_dbpedia_prompts.NER_PROMPT,\n",
        "                 URI_GENERATION_PROMPT: str = t2sparql_dbpedia_prompts.URI_GENERATION_PROMPT,\n",
        "                 SPARQL_GENERATION_PROMPT: str = t2sparql_dbpedia_prompts.SPARQL_GENERATION_PROMPT,\n",
        "                 QUERY_REPAIR_PROMPT: str = t2sparql_dbpedia_prompts.QUERY_REPAIR_PROMPT,\n",
        "                 QUESTION_CLARIFY: str = t2sparql_dbpedia_prompts.QUESTION_CLARIFY):\n",
        "\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.sparql_endpoint = SPARQLWrapper(dbpedia_endpoint)\n",
        "        self.sparql_endpoint.setReturnFormat(JSON)\n",
        "\n",
        "        self.NER_PROMPT = NER_PROMPT\n",
        "        self.URI_GENERATION_PROMPT = URI_GENERATION_PROMPT\n",
        "        self.SPARQL_GENERATION_PROMPT = SPARQL_GENERATION_PROMPT\n",
        "        self.QUERY_REPAIR_PROMPT = QUERY_REPAIR_PROMPT\n",
        "        self.QUESTION_CLARIFY = QUESTION_CLARIFY\n",
        "        self.rag = RAGSystem(['qald_9_plus_test_dbpedia.json', 'qald_9_plus_train_dbpedia.json', 'train-data.json'])\n",
        "\n",
        "    def translate_to_english(self, text):\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a precise translation engine. Translate the input to English exactly, preserving: \"\n",
        "                               \"1. All named entities (names, places, titles) \"\n",
        "                               \"2. Technical terms \"\n",
        "                               \"3. Question structure \"\n",
        "                               \"Output ONLY the translation without commentary.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Translate this to English exactly:\\n\\n{text}\"\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=500\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "\n",
        "    def get_dbpedia(self, text: str, language: str = \"en\") -> Optional[Dict]:\n",
        "        \"\"\"Get entities from DBpedia Spotlight API\"\"\"\n",
        "\n",
        "        # Отключаем предупреждения о небезопасных запросах\n",
        "        requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
        "\n",
        "        endpoint = f\"https://api.dbpedia-spotlight.org/{language}/annotate\"\n",
        "        headers = {\"Accept\": \"application/json\"}\n",
        "        params = {\"text\": text, \"confidence\": 0.5}\n",
        "\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                endpoint,\n",
        "                headers=headers,\n",
        "                data=params,\n",
        "                verify=False  # Disable SSL verification\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            #print(f\"DBpedia Spotlight API error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def uris(self, new_question: str) -> Tuple[Optional[str], Dict[str, str]]:\n",
        "        \"\"\"New entity extraction using DBpedia Spotlight\"\"\"\n",
        "\n",
        "\n",
        "        spotlight_result = self.get_dbpedia(new_question)\n",
        "        #print('Got spotlight result')\n",
        "\n",
        "        if spotlight_result and \"Resources\" in spotlight_result:\n",
        "            entities = {}\n",
        "            tagged_parts = []\n",
        "            remaining_text = new_question\n",
        "\n",
        "            # Process each found entity\n",
        "            for resource in sorted(spotlight_result[\"Resources\"], key=lambda x: -int(x[\"@offset\"])):\n",
        "                entity_text = resource[\"@surfaceForm\"]\n",
        "                entity_type = resource[\"@types\"].split(\",\")[0].split(\":\")[-1] if resource[\"@types\"] else \"thing\"\n",
        "                entity_uri = resource['@URI']\n",
        "\n",
        "                # Replace in remaining text\n",
        "                if entity_text in remaining_text:\n",
        "                    entities[entity_text] = entity_uri\n",
        "                    tagged_parts.append((remaining_text.index(entity_text),\n",
        "                                        f\"<{entity_text}>\"))\n",
        "                    remaining_text = remaining_text.replace(entity_text, \"\", 1)\n",
        "\n",
        "            # Reconstruct tagged question\n",
        "            if len(tagged_parts) > 1:\n",
        "\n",
        "                #print('Making tagged question + URIs')\n",
        "\n",
        "                # Sort by original position\n",
        "                tagged_parts.sort()\n",
        "                tagged_question = \"\"\n",
        "                last_pos = 0\n",
        "\n",
        "                for pos, tag in tagged_parts:\n",
        "                    tagged_question += remaining_text[last_pos:pos] + tag\n",
        "                    last_pos = pos\n",
        "\n",
        "                tagged_question += remaining_text[last_pos:]\n",
        "\n",
        "                return tagged_question, entities\n",
        "            else:\n",
        "\n",
        "              #print('Tagged entities <= 1')\n",
        "              tagged_question, entities = self._original_extract_entities(new_question)\n",
        "              if not tagged_question:\n",
        "                return {\"error\": \"Failed to extract entities\"}\n",
        "              return tagged_question, self._original_generate_uris(tagged_question, entities)\n",
        "        else:\n",
        "          #print('Spotlight result is empty')\n",
        "          tagged_question, entities = self._original_extract_entities(new_question)\n",
        "          if not tagged_question:\n",
        "            return {\"error\": \"Failed to extract entities\"}\n",
        "          return tagged_question, self._original_generate_uris(tagged_question, entities)\n",
        "\n",
        "    def _original_extract_entities(self, question: str) -> Tuple[Optional[str], Dict[str, str]]:\n",
        "        \"\"\"Original GPT-4 based entity extraction (kept as fallback)\"\"\"\n",
        "        prompt = f\"\"\"{self.NER_PROMPT}\\n\\nQuestion: {question}\\nProvide output in the exact required format:\"\"\"\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"\"\"You are an expert named entity recognizer for DBpedia questions. For each input question, follow this exact thinking process:\n",
        "\n",
        "                        1. ANALYZE the question structure:\n",
        "                          \"In the question [quote question], we are asked: [paraphrase]\"\n",
        "\n",
        "                        2. IDENTIFY components:\n",
        "                          - Main subject (class/resource)\n",
        "                          - Properties/relationships\n",
        "                          - Concrete entities (people/places/works)\n",
        "                          - Constraints/conditions\n",
        "\n",
        "                        3. EXTRACT entities:\n",
        "                          \"so we need to identify: [list entity types]\"\n",
        "                          \"The entities are: [list specific entities]\"\n",
        "\n",
        "                        4. GENERATE intermediary question:\n",
        "                          \"So the intermediary_question is: [exact format as examples]\"\n",
        "\n",
        "                        Output MUST follow this exact template for every question:\n",
        "\n",
        "                        Let's think step by step. In the question \"[original question]\", we are asked: \"[paraphrased question]\".\n",
        "                        so we need to identify: [entity types].\n",
        "                        The entities are: [specific entities].\n",
        "                        So the intermediary_question is: [exact match to example format]\"\"\"\n",
        "\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": prompt\n",
        "                    }\n",
        "                ],\n",
        "                temperature=0.0,\n",
        "                max_tokens=400\n",
        "            )\n",
        "\n",
        "            full_response = response.choices[0].message.content.strip()\n",
        "            #print(full_response)\n",
        "\n",
        "            intermediary_match = re.search(\n",
        "                r'So the intermediary_question is:\\s*(.*?)$',\n",
        "                full_response,\n",
        "                re.MULTILINE\n",
        "            )\n",
        "\n",
        "            if not intermediary_match:\n",
        "                #print(\"Error: Couldn't extract intermediary question from response\")\n",
        "                return None, {}\n",
        "\n",
        "            tagged_question = intermediary_match.group(1).strip()\n",
        "\n",
        "            entities = {}\n",
        "            for match in re.finditer(r'<([^>]+)>([^<]+)</\\1>', tagged_question):\n",
        "                entity_type, entity_value = match.groups()\n",
        "                entities[entity_value] = entity_type\n",
        "\n",
        "            if not re.match(r'^Let\\'s think step by step\\.', full_response):\n",
        "                #print(\"Error: Response doesn't follow DINSQL format\")\n",
        "                return None, {}\n",
        "\n",
        "            return tagged_question, entities\n",
        "\n",
        "        except Exception as e:\n",
        "            #print(f\"Error in entity extraction: {str(e)}\")\n",
        "            return None, {}\n",
        "\n",
        "    def _original_generate_uris(self, tagged_question: str, entities: Dict[str, str]) -> Dict[str, str]:\n",
        "        entity_list = \"\\n\".join([f\"- {value} ({type})\" for value, type in entities.items()])\n",
        "        prompt = f\"{self.URI_GENERATION_PROMPT}\\n\\nTagged question: {tagged_question}\\nEntities:\\n{entity_list}\\n\\nDBpedia URIs:\"\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"\"\"\n",
        "                    DBpedia URI Conversion Expert - Entity to URI Mapping\n",
        "\n",
        "                      Input Requirements:\n",
        "                      1. Original question (for context)\n",
        "                      2. Intermediary question with marked entities (<entity_type>)\n",
        "\n",
        "                      Output Format:\n",
        "                      - <tagged_entity> : full_DBpedia_URI (one per line)\n",
        "\n",
        "                      URI Selection Rules:\n",
        "\n",
        "                      1. Determining Entity Type:\n",
        "\n",
        "                        a) RESOURCES (Specific named entities):\n",
        "                            - Indicators: Proper nouns, specific instances\n",
        "                            - Examples: <Blanche DuBois>, <Mid Wales>, <Python_(language)>\n",
        "                            - Format: http://dbpedia.org/resource/Exact_Name\n",
        "                              - Preserve original capitalization\n",
        "                              - Replace spaces with underscores\n",
        "                              - Keep special characters (parentheses, commas)\n",
        "                              - Use official DBpedia names (check redirects)\n",
        "\n",
        "                        b) CLASSES (Categories/Types):\n",
        "                            - Indicators: Generic categories, answers \"what kind?\"\n",
        "                            - Examples: <play>, <company>, <city>\n",
        "                            - Format: http://dbpedia.org/ontology/ProperCase\n",
        "                              - Always singular form\n",
        "                              - Capitalize first letter\n",
        "                              - Use most specific available class\n",
        "\n",
        "                        c) PROPERTIES (Relationships):\n",
        "                            - Indicators: Connects entities, shows relationships\n",
        "                            - Examples: <founded by>, <alma mater>, <developer>\n",
        "                            - Selection Priority:\n",
        "                              1. Ontology properties (preferred):\n",
        "                                  - Format: http://dbpedia.org/ontology/lowercase_property\n",
        "                                  - More stable, better defined semantics\n",
        "                              2. Generic properties (fallback):\n",
        "                                  - Format: http://dbpedia.org/property/lowercase_property\n",
        "                                  - Used when no ontology property exists\n",
        "                            - Transformation rules:\n",
        "                              - Convert to lowercase\n",
        "                              - Replace spaces with underscores\n",
        "                              - Use natural property names (e.g., 'alma mater' : 'almaMater')\n",
        "                \"\"\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=0.0,\n",
        "                max_tokens=500\n",
        "            )\n",
        "            uri_text = response.choices[0].message.content\n",
        "\n",
        "            uris = {}\n",
        "            for line in uri_text.split('\\n'):\n",
        "                if line.strip() and ':' in line:\n",
        "                    parts = line.split(':', 1)\n",
        "                    entity = parts[0].strip().strip('-').strip()\n",
        "                    uri = parts[1].strip()\n",
        "                    uris[entity] = uri\n",
        "\n",
        "            return uris\n",
        "        except Exception as e:\n",
        "            #print(f\"Error in URI generation: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def generate_sparql(self, original_question: str, tagged_question: str, uris: Dict[str, str]) -> Optional[str]:\n",
        "\n",
        "        uri_mapping = \"\\n\".join([f\"- <{entity}> : {uri}\" for entity, uri in uris.items()])\n",
        "        URI = [uri for entity, uri in uris.items()]\n",
        "\n",
        "        dbpedia_neighbors = [self.get_dbpedia_neighbors(f'{entity_url}') for entity_url in URI]\n",
        "\n",
        "        # for each URI got maximum 10 neighbours, but total amount of neighbours must be <= 30\n",
        "\n",
        "        max_neighbours_per_entity = 30 // len(URI)\n",
        "\n",
        "        dbpedia_neighbors_uris = []\n",
        "\n",
        "        for neighbour_dict in dbpedia_neighbors:\n",
        "            dbpedia_neighbors_uris += list(neighbour_dict.values())[:max_neighbours_per_entity]\n",
        "\n",
        "        context_from_rag = self.rag.get_context(original_question, top_k=7)\n",
        "\n",
        "        prompt = f\"\"\"{self.SPARQL_GENERATION_PROMPT}\\n\\n\n",
        "                Input:\n",
        "                Original Question: \"{original_question}\"\n",
        "                Question with Entities: \"{tagged_question}\"\n",
        "                DBpedia URIs:\n",
        "                {uri_mapping}\n",
        "                dbpedia_neighbors:\n",
        "                {' '.join(dbpedia_neighbors_uris)}\n",
        "                Similar questions from datasets and correct SPARQL for them for the better context: {context_from_rag}\n",
        "                \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"\"\"You are a precise SPARQL query generator that follows DINSQL-style reasoning with URI generation capabilities. Strictly follow these rules:\n",
        "\n",
        "                          0. OUTPUT CONTROL:\n",
        "                          - Return ONLY the SPARQL query in a clean code block OR \"INVALID_INPUT\"\n",
        "                          - Absolutely NO explanations, thought processes, or disclaimers\n",
        "                          - If generating URIs, do so SILENTLY without commentary\n",
        "\n",
        "                          1. URI HANDLING RULES:\n",
        "                          1.1 For provided URIs:\n",
        "                          - Use EXACTLY as given\n",
        "                          - Never modify existing URIs\n",
        "                          1.2 For missing URIs:\n",
        "                          - Generate ONLY when ALL conditions are met:\n",
        "                            * The URI is CRITICAL for query execution\n",
        "                            * The needed URI is OBVIOUS (e.g., dbo:birthDate for person)\n",
        "                            * The URI follows standard DBpedia/Wikidata patterns\n",
        "                          - Generation priority:\n",
        "                            * Properties > Classes > Entities\n",
        "                            * Never generate entity URIs (only properties/classes)\n",
        "\n",
        "                          2. QUERY CONSTRUCTION:\n",
        "                          2.1 Structure analysis:\n",
        "                          - Parse original question intent\n",
        "                          - Map all tagged relationships\n",
        "                          - Incorporate ALL provided URIs\n",
        "                          2.2 Transformation rules:\n",
        "                          - Preserve exact entity relationships\n",
        "                          - Maintain variable binding consistency\n",
        "                          - Apply proper class restrictions (rdf:type)\n",
        "                          2.3 Best practices:\n",
        "                          - Use SELECT/ASK appropriately\n",
        "                          - Include DISTINCT when needed\n",
        "                          - Use proper SPARQL syntax\n",
        "\n",
        "                          3. VALIDATION CHECKS:\n",
        "                          - All provided URIs must appear exactly\n",
        "                          - Generated URIs must follow standard patterns\n",
        "                          - Variables must be properly joined\n",
        "                          - Query must execute as intended\n",
        "\n",
        "                          4. FAILURE MODE:\n",
        "                          - Return \"INVALID_INPUT\" ONLY if:\n",
        "                            * Missing CRITICAL entity URIs\n",
        "                            * Question is fundamentally unanswerable\n",
        "                            * Syntax cannot be fixed\n",
        "\n",
        "                          5. Output format:\n",
        "                            - Always begin with a 'Thought Process:' section explaining:\n",
        "                              * How you interpreted the question\n",
        "                              * Why you chose specific patterns\n",
        "                              * How variables connect\n",
        "                            - Provide the SPARQL query in a clean code block\n",
        "\n",
        "                          Examples of allowed URI generation:\n",
        "                          - \"birth year\" → dbo:birthYear\n",
        "                          - \"company founder\" → dbo:founder\n",
        "                          - \"chemical formula\" → dbo:formula\n",
        "\n",
        "                          Examples of forbidden generation:\n",
        "                          - Inventing entity URIs (e.g., dbr:SomePerson)\n",
        "                          - Guessing non-obvious properties\n",
        "                          - Creating non-standard prefixes\n",
        "\n",
        "                          REMEMBER:\n",
        "                          - When in doubt, GENERATE rather than fail\n",
        "                          - Better a working query than INVALID_INPUT\"\"\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": prompt\n",
        "                    }\n",
        "                ],\n",
        "                temperature=0.0,\n",
        "                max_tokens=800\n",
        "            )\n",
        "\n",
        "            full_response = response.choices[0].message.content.strip()\n",
        "            sparql_match = re.search(r'SPARQL:\\s*(.*?)$', full_response, re.DOTALL)\n",
        "            if not sparql_match:\n",
        "              correction_response = self.client.chat.completions.create(\n",
        "              model=\"gpt-4\",\n",
        "              messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"\"\"You are a SPARQL error correction expert. When given a failed generation attempt:\n",
        "                    1. Analyze ALL problems\n",
        "                    2. Explain each issue clearly\n",
        "                    3. Return a WORKING SPARQL query\n",
        "                    4. Use ONLY the provided URIs\n",
        "                    5. Maintain strict DINSQL standards\"\"\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"\"\"Please analyze and fix the SPARQL generation error for this task:\n",
        "\n",
        "                                    Original Question: \"{original_question}\"\n",
        "                                    Tagged Question: \"{tagged_question}\"\n",
        "                                    Provided URIs:\n",
        "                                    {uri_mapping}\n",
        "\n",
        "                                    Required Actions:\n",
        "                                    1. Identify ALL issues in the failed generation attempt\n",
        "                                    2. Explain each problem clearly\n",
        "                                    3. Provide the CORRECTED SPARQL query\n",
        "                                    4. Ensure the fixed query:\n",
        "                                      - Uses all provided URIs correctly\n",
        "                                      - Matches the question intent\n",
        "                                      - Has proper syntax and structure\n",
        "\n",
        "                                    Agent reasoning:\n",
        "                                    {full_response}\n",
        "\n",
        "                                    Return your response in EXACTLY this format:\n",
        "\n",
        "                                    ANALYSIS:\n",
        "                                    1. [Issue 1 description]\n",
        "                                    2. [Issue 2 description]...\n",
        "\n",
        "                                    CORRECTED SPARQL:\n",
        "                                    [The fully corrected SPARQL query here]\"\"\"\n",
        "                }\n",
        "              ],\n",
        "              temperature=0.0,\n",
        "              max_tokens=1000\n",
        "              )\n",
        "\n",
        "              correction_text = correction_response.choices[0].message.content\n",
        "\n",
        "              # Извлекаем исправленный SPARQL\n",
        "              corrected_sparql = re.search(r'CORRECTED SPARQL:\\s*(.*?)$', correction_text, re.DOTALL) or \\\n",
        "                                re.search(r'```sparql\\n(.*?)```', correction_text, re.DOTALL)\n",
        "              if corrected_sparql:\n",
        "                  return corrected_sparql.group(1).strip()\n",
        "              else:\n",
        "                  return None\n",
        "            else:\n",
        "                sparql_query = sparql_match.group(1).strip()\n",
        "                return sparql_query\n",
        "\n",
        "        except Exception as e:\n",
        "           return None\n",
        "\n",
        "    def postprocess_query(self, query) -> str:\n",
        "        query = re.sub(r'^\\s*#.*$', '', query, flags=re.MULTILINE)\n",
        "        return query.strip()\n",
        "\n",
        "    def validate_query(self, query):\n",
        "      try:\n",
        "          self.sparql_endpoint.setQuery(query)\n",
        "          results = self.sparql_endpoint.query().convert()\n",
        "\n",
        "          # Проверка на пустые результаты\n",
        "          if 'boolean' in results.keys():\n",
        "              if isinstance(results['boolean'], bool):\n",
        "                  return True, None\n",
        "          elif 'results' in results:\n",
        "              if len(results['results']['bindings']) == 0:\n",
        "                  return False, \"Query executed successfully but returned empty results. Please regenerate the query with different parameters or conditions.\"\n",
        "          return True, None\n",
        "\n",
        "      except Exception as e:\n",
        "          error_msg = re.sub(r\"Endpoint returned:.*\", \"\", str(e)).strip()\n",
        "          return False, error_msg\n",
        "\n",
        "    def repair_query(self, original_query, error, context):\n",
        "        prompt = self.QUERY_REPAIR_PROMPT.format(\n",
        "          error=error,\n",
        "          original_query=original_query,\n",
        "          original_question=context['original_question'],\n",
        "          tagged_question=context['tagged_question'],\n",
        "          uris=\"\\n\".join([f\"- {k}: {v}\" for k,v in context['uris'].items()])\n",
        "        )\n",
        "        try:\n",
        "          response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"\"\"You are a SPARQL repair expert. Apply these strategies:\n",
        "                  1. For empty results:\n",
        "                    - Find alternative URIs keeping original meaning\n",
        "                    - Use dbo: instead of dbp: when possible\n",
        "                    - Try superclasses/subproperties\n",
        "                  2. For syntax errors:\n",
        "                    - Fix exactly what's broken\n",
        "                    - Never change working parts\n",
        "                  Output ONLY the fixed query\"\"\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.3,  # Немного креативности для альтернативных URI\n",
        "            max_tokens=600\n",
        "            )\n",
        "          return response.choices[0].message.content.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            return None\n",
        "\n",
        "    def get_dbpedia_neighbors(self, entity_url: str):\n",
        "        \"\"\"\n",
        "        Extracts all neighbours of an entity in DBpedia knowledge graph.\n",
        "\n",
        "        Args:\n",
        "            entity_url (str): URL сущности в DBpedia (например, \"http://dbpedia.org/resource/Danielle_Steel\")\n",
        "        Returns:\n",
        "            dict: Словарь, где ключи - имена связанных сущностей, значения - их URL в DBpedia\n",
        "        \"\"\"\n",
        "        # Проверяем и корректируем URL\n",
        "\n",
        "        if not entity_url.startswith(\"http://dbpedia.org/resource/\"):\n",
        "            entity_url = f\"http://dbpedia.org/resource/{entity_url.split('/')[-1]}\"\n",
        "\n",
        "\n",
        "        # SPARQL запрос для получения всех соседей\n",
        "        sparql_query = f\"\"\"\n",
        "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "            SELECT DISTINCT ?property ?neighbor ?neighborLabel WHERE {{\n",
        "              <{entity_url}> ?property ?neighbor .\n",
        "              FILTER (isURI(?neighbor) && STRSTARTS(STR(?neighbor), \"http://dbpedia.org/resource/\"))\n",
        "              OPTIONAL {{\n",
        "                ?neighbor rdfs:label ?neighborLabel .\n",
        "                FILTER (LANG(?neighborLabel) = \"en\")\n",
        "              }}\n",
        "            }}\n",
        "            LIMIT 30\n",
        "            \"\"\"\n",
        "\n",
        "        # Параметры запроса\n",
        "        params = {\n",
        "            'query': sparql_query,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        headers = {\n",
        "            'Accept': 'application/sparql-results+json'\n",
        "        }\n",
        "\n",
        "        endpoint_url = \"https://dbpedia.org/sparql\"\n",
        "\n",
        "        try:\n",
        "            response = requests.get(endpoint_url, params=params, headers=headers, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            results = response.json()\n",
        "\n",
        "            neighbors = {}\n",
        "\n",
        "            for binding in results[\"results\"][\"bindings\"]:\n",
        "\n",
        "              if len(list(neighbors.keys())) <= 10:\n",
        "                neighbor_url = binding[\"neighbor\"][\"value\"]\n",
        "\n",
        "                # Используем URL как имя, если нет метки\n",
        "                neighbor_name = binding.get(\"neighborLabel\", {}).get(\"value\", neighbor_url.split(\"/\")[-1].replace(\"_\", \" \"))\n",
        "                neighbors[neighbor_name] = neighbor_url\n",
        "              else:\n",
        "                break\n",
        "\n",
        "            return neighbors\n",
        "\n",
        "        except Exception as e:\n",
        "            return {}\n",
        "\n",
        "    def query_rewriting(self, question: str) -> str:\n",
        "        prompt = self.QUESTION_CLARIFY.format(question=question)\n",
        "\n",
        "        try:\n",
        "          response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"\"\"You are a SPARQL repair expert. Apply these strategies:\n",
        "                  1. Try to change the input question as little as possible\n",
        "                  2. If the given question is correct (not ambiguous) print it without changes.\n",
        "                  3. As the output put only the clarified question without writing the given one.\"\"\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=600\n",
        "            )\n",
        "          return response.choices[0].message.content.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            return None\n",
        "\n",
        "    def execute_pipeline(self, question: str, max_retries: int = 2) -> Dict:\n",
        "\n",
        "        # Step 1) Translating\n",
        "        en_question = self.translate_to_english(question)\n",
        "        print('English question: ', en_question)\n",
        "\n",
        "        # Step 2) Query rewriting\n",
        "        rewritten_question = self.query_rewriting(en_question)\n",
        "        print('Rewritten question: ', rewritten_question)\n",
        "\n",
        "        # Step 3) Entity recognition\n",
        "        tagged_question, entities_URI = self.uris(rewritten_question)\n",
        "\n",
        "        # Step 4) SPARQL generation\n",
        "        sparql = self.generate_sparql(en_question, tagged_question, entities_URI)\n",
        "\n",
        "        if not sparql:\n",
        "            return {\"error\": \"Failed to generate SPARQL\", \"tagged_question\": tagged_question, \"uris\": entities_URI}\n",
        "\n",
        "        # Step 5) SPARQL repairing\n",
        "        for attempt in range(max_retries + 1):\n",
        "            is_valid, error = self.validate_query(sparql)\n",
        "            if is_valid:\n",
        "                return {\n",
        "                    \"status\": \"success\",\n",
        "                    \"tagged_question\": tagged_question,\n",
        "                    \"uris\": entities_URI,\n",
        "                    \"sparql\": sparql\n",
        "                }\n",
        "\n",
        "            if attempt < max_retries:\n",
        "                context = {\n",
        "                    \"original_question\": en_question,\n",
        "                    \"tagged_question\": tagged_question,\n",
        "                    \"uris\": entities_URI\n",
        "                }\n",
        "                repaired_query = self.repair_query(sparql, error, context)\n",
        "                if repaired_query and repaired_query != sparql:\n",
        "                    continue\n",
        "\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"error\": error,\n",
        "                \"tagged_question\": tagged_question,\n",
        "                \"uris\": entities_URI,\n",
        "                \"sparql\": sparql\n",
        "            }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhvg0QMsX6bP"
      },
      "source": [
        "# [Corporate]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O19mjMBxZKpX"
      },
      "outputs": [],
      "source": [
        "class GPTEnhancedSemanticSearcher:\n",
        "    def __init__(self, openai_api_key: str):\n",
        "        self.client = OpenAI(api_key=openai_api_key)\n",
        "        self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "        self.graph = None\n",
        "        self.chunks = []\n",
        "        self.metadata = []\n",
        "        self.index = None\n",
        "\n",
        "    def translate_to_english(self, text):\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a precise translation engine. Translate the input to English exactly, preserving: \"\n",
        "                               \"1. All named entities (names, places, titles) \"\n",
        "                               \"2. Technical terms \"\n",
        "                               \"3. Question structure \"\n",
        "                               \"Output ONLY the translation without commentary.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Translate this to English exactly:\\n\\n{text}\"\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=500\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "\n",
        "    def load_ttl(self, file_paths):\n",
        "        self.graph = Graph()\n",
        "        for path in file_paths:\n",
        "            self.graph.parse(path, format=\"turtle\")\n",
        "\n",
        "    def create_chunks(self):\n",
        "        self.chunks = []\n",
        "        self.metadata = []\n",
        "\n",
        "        for entity in set(self.graph.subjects()):\n",
        "            if isinstance(entity, (URIRef, BNode)):\n",
        "                desc = \"\\n\".join([f\"{self._uri_to_sparql(p)} {self._uri_to_sparql(o)}\"\n",
        "                                for _, p, o in self.graph.triples((entity, None, None))])\n",
        "                self.chunks.append(f\"Entity: {self._uri_to_sparql(entity)}\\n{desc}\")\n",
        "                self.metadata.append({\"source\": str(entity)})\n",
        "\n",
        "    def _uri_to_sparql(self, uri):\n",
        "        if isinstance(uri, URIRef):\n",
        "            for prefix, ns in self.graph.namespaces():\n",
        "                if uri.startswith(ns):\n",
        "                    return f\"{prefix}:{uri.replace(ns, '')}\"\n",
        "        return f\"<{uri}>\"\n",
        "\n",
        "    def build_index(self):\n",
        "        self.embeddings = self.model.encode(self.chunks, show_progress_bar=True)\n",
        "        self.index = IndexFlatIP(self.embeddings.shape[1])\n",
        "        self.index.add(self.embeddings)\n",
        "\n",
        "    def search(self, query: str, top_k: int = 3):\n",
        "        query_embedding = self.model.encode(query)\n",
        "        query_embedding = np.array(query_embedding, dtype=np.float32).reshape(1, -1)\n",
        "\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "        faiss.normalize_L2(self.embeddings)\n",
        "\n",
        "        distances, indices = self.index.search(query_embedding, top_k)\n",
        "\n",
        "        return [{\n",
        "            \"text\": self.chunks[idx],\n",
        "            \"metadata\": self.metadata[idx],\n",
        "            \"score\": float(distances[0][i])\n",
        "        } for i, idx in enumerate(indices[0])]\n",
        "\n",
        "    def generate_sparql(self, original_question: str, top_k: int = 3) -> Optional[dict]:\n",
        "        # Шаг 1: Перевод вопроса\n",
        "        translated_question = self.translate_to_english(original_question)\n",
        "\n",
        "        # Шаг 2: Семантический поиск\n",
        "        rag_results = self.search(translated_question, top_k)\n",
        "        context = \"\\n\".join([res['text'] for res in rag_results])\n",
        "\n",
        "        # Шаг 3: Генерация SPARQL\n",
        "        namespaces = [f\"PREFIX {prefix}: <{ns}>\" for prefix, ns in self.graph.namespaces()]\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        It is required to generate a SPARQL query for a specific knowledge graph.\n",
        "        The graph is loaded from TTL files and has the following characteristics:\n",
        "\n",
        "        {len(namespaces)} registered namespace:\n",
        "        {chr(10).join(namespaces)}\n",
        "\n",
        "        A question for generating a query:\n",
        "        \"{translated_question}\"\n",
        "\n",
        "        Relevant graph fragments:\n",
        "        {context}\n",
        "\n",
        "        Strict requirements:\n",
        "        1. Use ONLY prefixes and URIs from the given context\n",
        "        2. Don't invent new properties/classes.\n",
        "        3. Match the exact meaning of the question\n",
        "        4. Output format:\n",
        "        ```sparql\n",
        "        YOUR_QUERY\n",
        "        ```\n",
        "\n",
        "        Additional notes:\n",
        "        - If the issue cannot be resolved based on the context, return \"INVALID\"\n",
        "        - Always filter by rdf:type if the entity class is known\n",
        "        - Use DISTINCT to avoid duplicates\n",
        "        - Optimize the query for fast execution\n",
        "        \"\"\"\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"\"\"You are an expert SPARQL query generator. Your task is to create a working query for a specific knowledge graph.\n",
        "\n",
        "                    Critical rules:\n",
        "                    1. Follow the ontology exactly from the context\n",
        "                    2. Keep the original namespace prefixes\n",
        "                    3. Verify the validity of all URIs used\n",
        "                    4. Optimize the query structure\n",
        "\n",
        "                    The algorithm of operation:\n",
        "                    1. Analyze the question and the context\n",
        "                    2. Identify all necessary URIs\n",
        "                    3. Build the appropriate query pattern\n",
        "                    4. Verify compliance with the requirements\n",
        "                    5. Return a clean SPARQL or \"INVALID\"\n",
        "\n",
        "                    \"\"\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=1000\n",
        "        )\n",
        "\n",
        "        raw_output = response.choices[0].message.content.strip()\n",
        "\n",
        "        if \"```sparql\" in raw_output:\n",
        "            generated_sparql = raw_output.split(\"```sparql\")[1].split(\"```\")[0].strip()\n",
        "        else:\n",
        "            generated_sparql = raw_output\n",
        "\n",
        "        return {\n",
        "            \"generated_sparql\": generated_sparql\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LP938CFcHrh"
      },
      "outputs": [],
      "source": [
        "searcher = GPTEnhancedSemanticSearcher(openai_api_key=api_key)\n",
        "\n",
        "searcher.load_ttl([\"/content/prod-inst.ttl\", \"/content/prod-vocab.ttl\"])\n",
        "\n",
        "searcher.create_chunks()\n",
        "searcher.build_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBcoOCm9i4pz"
      },
      "outputs": [],
      "source": [
        "result = searcher.generate_sparql(\"Which products belong to the Hardware category?\", top_k=25)\n",
        "sparql = result['generated_sparql']\n",
        "sparql"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQNSACdNTz5e"
      },
      "source": [
        "# QALD Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK1aePsN6sAq"
      },
      "source": [
        "## Preparing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEtjfqRUQYjZ"
      },
      "outputs": [],
      "source": [
        "def extract_values(data):\n",
        "\n",
        "    values = []\n",
        "    for item in data:\n",
        "        if isinstance(item, dict):\n",
        "            for key, nested in item.items():\n",
        "                if isinstance(nested, dict) and 'value' in nested:\n",
        "                    values.append(nested['value'])\n",
        "                elif isinstance(nested, (list, tuple)):\n",
        "                    for element in nested:\n",
        "                        if isinstance(element, dict) and 'value' in element:\n",
        "                            values.append(element['value'])\n",
        "    return values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uDCefhaH4Q0"
      },
      "outputs": [],
      "source": [
        "endpoint=\"http://dbpedia.org/sparql\"\n",
        "#query = \"\"\"SELECT DISTINCT ?uri WHERE { ?x <http://dbpedia.org/property/school> <http://dbpedia.org/resource/Pietermaritzburg> . ?x <http://dbpedia.org/property/school> ?uri  . }\n",
        "#\"\"\"\n",
        "#sparql = SPARQLWrapper(endpoint)\n",
        "#sparql.setQuery(query)\n",
        "#sparql.setReturnFormat(JSON)\n",
        "#results = sparql.query().convert()\n",
        "#bindings = results['results']['bindings']\n",
        "#bindings\n",
        "#extract_values(bindings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCfZqC7-IRmt"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"ASK WHERE { <http://dbpedia.org/resource/Hyundai_Lavita> <http://dbpedia.org/property/assembly> <http://dbpedia.org/resource/Ulsan> }\n",
        "\"\"\"\n",
        "sparql = SPARQLWrapper(endpoint)\n",
        "sparql.setQuery(query)\n",
        "sparql.setReturnFormat(JSON)\n",
        "results = sparql.query().convert()\n",
        "results\n",
        "#extract_values(bindings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqEVawSVKF_F"
      },
      "outputs": [],
      "source": [
        "isinstance(results['boolean'], bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLgl80AgF68K"
      },
      "outputs": [],
      "source": [
        "def sparql_results_match(query1, query2, endpoint=\"http://dbpedia.org/sparql\"):\n",
        "    def execute_query(query):\n",
        "        sparql = SPARQLWrapper(endpoint)\n",
        "        sparql.setQuery(query)\n",
        "        sparql.setReturnFormat(JSON)\n",
        "        try:\n",
        "            results = sparql.query().convert()\n",
        "\n",
        "            # Обработка ASK запросов (возвращают boolean)\n",
        "            if 'boolean' in results:\n",
        "                return results['boolean']\n",
        "\n",
        "            # Обработка SELECT запросов\n",
        "            if 'results' in results:\n",
        "                bindings = results['results']['bindings']\n",
        "                return extract_values(bindings)\n",
        "\n",
        "            # Для других типов запросов (CONSTRUCT/DESCRIBE)\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error executing query: {e}\")\n",
        "            return None\n",
        "\n",
        "    results1 = execute_query(query1)\n",
        "    results2 = execute_query(query2)\n",
        "    print(f\"Results1: {results1}\")\n",
        "    print(f\"Results2: {results2}\")\n",
        "\n",
        "    # Оба запроса вернули None (ошибка выполнения)\n",
        "    if results1 is None and results2 is None:\n",
        "        return True\n",
        "\n",
        "    if (results1 is None and results2 is not None) or (results1 is not None and results2 is None):\n",
        "        return False\n",
        "\n",
        "    # Обработка ASK запросов\n",
        "    if isinstance(results1, bool) and isinstance(results2, bool):\n",
        "        return results1 == results2\n",
        "\n",
        "    # Один из результатов - boolean, другой - нет\n",
        "    if isinstance(results1, bool) or isinstance(results2, bool):\n",
        "        return False\n",
        "\n",
        "    elif len(results1) > 0 and len(results2) > 0:\n",
        "      if results1 and results2:\n",
        "        return results1 == results2\n",
        "      else:\n",
        "        return False\n",
        "    else:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHuWdAQrR4_a"
      },
      "outputs": [],
      "source": [
        "with open('qald_9_plus_test_dbpedia.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "rows = []\n",
        "\n",
        "for question in data['questions']:\n",
        "    question_id = question['id']\n",
        "\n",
        "    for q in question['question']:\n",
        "        lang = q['language']\n",
        "        question_text = q['string']\n",
        "\n",
        "        sparql_query = question['query']['sparql']\n",
        "\n",
        "        answers = []\n",
        "        for answer in question['answers']:\n",
        "            if 'results' in answer:\n",
        "                for binding in answer['results']['bindings']:\n",
        "                    for var, value in binding.items():\n",
        "                        answers.append(value['value'])\n",
        "            elif 'boolean' in answer:\n",
        "                answers.append(str(answer['boolean']))\n",
        "\n",
        "        rows.append({\n",
        "            'id': question_id,\n",
        "            'language': lang,\n",
        "            'question': question_text,\n",
        "            'sparql_query': sparql_query,\n",
        "            'answers': ', '.join(answers) if answers else None\n",
        "        })\n",
        "\n",
        "test_df = pd.DataFrame(rows)\n",
        "test_df.to_csv('qald_questions.csv', index=False, encoding='utf-8')\n",
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDUORf3gR8NY"
      },
      "outputs": [],
      "source": [
        "test_df = test_df.sample(n=50)\n",
        "test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUOmBXcYMN6R"
      },
      "source": [
        "## Цикл тестирования"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hYqsCtfRyGT"
      },
      "outputs": [],
      "source": [
        "api_key = \"\"\n",
        "pipeline = DBpediaPipeline(api_key)\n",
        "\n",
        "total_questions = len(test_df)\n",
        "correct_em = 0\n",
        "\n",
        "for q in range(len(test_df)):\n",
        "\n",
        "    result = pipeline.execute_pipeline(test_df.iloc[q]['question'])\n",
        "    sparql = result['sparql']\n",
        "    gold_sparql = test_df.iloc[q][\"sparql_query\"]\n",
        "\n",
        "    generated_normalized = ' '.join(sparql.split()).lower()\n",
        "    gold_normalized = ' '.join(gold_sparql.split()).lower()\n",
        "\n",
        "    is_correct = sparql_results_match(sparql, gold_sparql)\n",
        "    if is_correct:\n",
        "      print(True)\n",
        "      correct_em += 1\n",
        "    #print(f\"Question: {test_df.iloc[q]['question']}\")\n",
        "    print(f\"Generated: {sparql}\")\n",
        "    print(f\"GOLD: {gold_sparql}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "em_accuracy = correct_em / total_questions\n",
        "print(f\"\\nMatch Accuracy: {em_accuracy:.2%} ({correct_em}/{total_questions})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp-mUCJNGvHm"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnMIBTOTEvZS"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nMatch Accuracy: {em_accuracy:.2%} ({correct_em}/{total_questions})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gk-5eSHahnl"
      },
      "source": [
        "## Отдельная проверка извлечения соседей по URI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPtDUB_n_q-k"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from urllib.parse import quote\n",
        "\n",
        "def get_dbpedia_neighbors(entity_url: str) -> dict:\n",
        "    \"\"\"\n",
        "    Извлекает всех соседей сущности в графе DBpedia.\n",
        "\n",
        "    Args:\n",
        "        entity_url (str): URL сущности в DBpedia (например, \"http://dbpedia.org/resource/Danielle_Steel\")\n",
        "\n",
        "    Returns:\n",
        "        dict: Словарь, где ключи - имена связанных сущностей, значения - их URL в DBpedia\n",
        "    \"\"\"\n",
        "    # Проверяем и корректируем URL\n",
        "    if not entity_url.startswith(\"http://dbpedia.org/resource/\"):\n",
        "        entity_url = f\"http://dbpedia.org/resource/{entity_url.split('/')[-1]}\"\n",
        "\n",
        "    # SPARQL запрос для получения всех соседей (исправленная версия)\n",
        "    sparql_query = f\"\"\"\n",
        "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "    SELECT DISTINCT ?property ?neighbor ?neighborLabel WHERE {{\n",
        "      <{entity_url}> ?property ?neighbor .\n",
        "      FILTER (isURI(?neighbor) && STRSTARTS(STR(?neighbor), \"http://dbpedia.org/resource/\"))\n",
        "      OPTIONAL {{\n",
        "        ?neighbor rdfs:label ?neighborLabel .\n",
        "        FILTER (LANG(?neighborLabel) = \"en\")\n",
        "      }}\n",
        "    }}\n",
        "    LIMIT 1000\n",
        "    \"\"\"\n",
        "\n",
        "    # Параметры запроса\n",
        "    params = {\n",
        "        'query': sparql_query,\n",
        "        'format': 'json'\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        'Accept': 'application/sparql-results+json'\n",
        "    }\n",
        "\n",
        "    endpoint_url = \"https://dbpedia.org/sparql\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(endpoint_url, params=params, headers=headers, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        results = response.json()\n",
        "\n",
        "        neighbors = {}\n",
        "        for binding in results[\"results\"][\"bindings\"]:\n",
        "            neighbor_url = binding[\"neighbor\"][\"value\"]\n",
        "            # Используем URL как имя, если нет метки\n",
        "            neighbor_name = binding.get(\"neighborLabel\", {}).get(\"value\", neighbor_url.split(\"/\")[-1].replace(\"_\", \" \"))\n",
        "            neighbors[neighbor_name] = neighbor_url\n",
        "\n",
        "        return neighbors\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при выполнении запроса: {str(e)}\")\n",
        "        if hasattr(e, 'response') and e.response:\n",
        "            print(f\"Ответ сервера: {e.response.text}\")\n",
        "        return {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lRmRtOaNA2-"
      },
      "outputs": [],
      "source": [
        "example_url = \"http://dbpedia.org/resource/Danielle_Steel\"\n",
        "print(f\"Получаем соседей для: {example_url}\")\n",
        "neighbors = get_dbpedia_neighbors(example_url)\n",
        "\n",
        "print(f\"\\nНайдено {len(neighbors)} связанных сущностей:\")\n",
        "for name, url in list(neighbors.items())[:20]:  # Выводим первые 20 для примера\n",
        "    print(f\"{name}: {url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zHV-2XSNCWm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "R2t0f3rdLUAw",
        "BK1aePsN6sAq",
        "4gk-5eSHahnl"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}